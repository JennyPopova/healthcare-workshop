{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/work-with-data/datasets-tutorial/labeled-datasets/labeled-datasets.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to labeled datasets\n",
    "\n",
    "Labeled datasets are output from Azure Machine Learning [labeling projects](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-create-labeling-projects). It captures the reference to the data (e.g. image files) and its labels. \n",
    "\n",
    "This tutorial introduces the capabilities of labeled datasets and how to use it in training.\n",
    "\n",
    "Learn how-to:\n",
    "\n",
    "> * Set up your development environment\n",
    "> * Explore labeled datasets\n",
    "> * Train a simple deep learning neural network\n",
    "\n",
    "## Prerequisite:\n",
    "* Understand the [architecture and terms](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture) introduced by Azure Machine Learning\n",
    "* Go through Azure Machine Learning [labeling projects](https://docs.microsoft.com/azure/machine-learning/service/how-to-create-labeling-projects) and export the labels as an Azure Machine Learning dataset\n",
    "* Go through the [configuration notebook](../../../configuration.ipynb) to:\n",
    "    * install the latest version of azureml-sdk\n",
    "    * install the latest version of azureml-contrib-dataset\n",
    "    * create a workspace and its configuration file (`config.json`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import azureml.core\n",
    "import azureml.contrib.dataset\n",
    "from azureml.core import Dataset, Workspace, Experiment\n",
    "from azureml.contrib.dataset import FileHandlingOption\n",
    "\n",
    "# check core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)\n",
    "print(\"Azure ML Contrib Version\", azureml.contrib.dataset.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to workspace\n",
    "\n",
    "Create a workspace object from the existing workspace. `Workspace.from_config()` reads the file **config.json** and loads the details into an object named `workspace`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load workspace\n",
    "workspace = Workspace.from_config()\n",
    "print('Workspace name: ' + workspace.name, \n",
    "      'Azure region: ' + workspace.location, \n",
    "      'Subscription id: ' + workspace.subscription_id, \n",
    "      'Resource group: ' + workspace.resource_group, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create experiment\n",
    "\n",
    "Create an experiment to track the runs in your workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an ML experiment\n",
    "experiment = Experiment(workspace=workspace, name='labeled-datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore labeled datasets\n",
    "\n",
    "**Note**: How to create labeled datasets is not covered in this tutorial. To create labeled datasets, you can go through [labeling projects](https://docs.microsoft.com/azure/machine-learning/service/how-to-create-labeling-projects) and export the output labels as Azure Machine Lerning datasets. \n",
    "\n",
    "`malaria_labels` used in this tutorial section is the output from a labeling project, with the task type of \"Object Identification\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get animal_labels dataset from the workspace\n",
    "malaria_labels = Dataset.get_by_name(workspace, 'malaria_20200928_173440')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeled dataset to pandas\n",
    "You can load labeled datasets into pandas DataFrame. There are 3 file handling option that you can choose to load the data files referenced by the labeled datasets:\n",
    "* Streaming: The default option to load data files.\n",
    "* Download: Download your data files to a local path.\n",
    "* Mount: Mount your data files to a mount point. Mount only works for Linux-based compute, including Azure Machine Learning notebook VM and Azure Machine Learning Compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "malaria_pd = malaria_labels.to_pandas_dataframe(file_handling_option=FileHandlingOption.MOUNT)\n",
    "malaria_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#malaria_pd = malaria_labels.to_pandas_dataframe(file_handling_option=FileHandlingOption.DOWNLOAD, target_path='./download/', overwrite_download=True)\n",
    "#malaria_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# read images from downloaded path\n",
    "img = mpimg.imread(malaria_pd.loc[0,'image_url'])\n",
    "imgplot = plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeled dataset to torchvision\n",
    "You can also load labeled datasets into [torchvision datasets](https://pytorch.org/docs/stable/torchvision/datasets.html), so that you can leverage on the open source libraries provided by PyTorch for image transformation and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import functional as F\n",
    "\n",
    "# load animal_labels dataset into torchvision dataset\n",
    "pytorch_dataset = malaria_labels.to_torchvision()\n",
    "img = pytorch_dataset[0][0]\n",
    "print(type(img))\n",
    "\n",
    "# use methods from torchvision to transform the img into grayscale\n",
    "pil_image = F.to_pil_image(img)\n",
    "gray_image = F.to_grayscale(pil_image, num_output_channels=3)\n",
    "\n",
    "imgplot = plt.imshow(gray_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prepearaion\n",
    "\n",
    "Data Preperation: We will make data and labels list where data will be image to array implementatation which contains RGB values of each image. and label will be class of cells here I will be taking 0 and 1 for two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "labels=[]\n",
    "for row in malaria_pd.itertuples():\n",
    "    try:\n",
    "        image=cv2.imread(row.image_url)\n",
    "        image_from_array = Image.fromarray(image, 'RGB')\n",
    "        size_image = image_from_array.resize((50, 50))\n",
    "        data.append(np.array(size_image))\n",
    "        if row.label == \"Malaria\":\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "        labels.append(label)\n",
    "    except AttributeError:\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cells=np.array(data)\n",
    "labels=np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=np.arange(Cells.shape[0])\n",
    "np.random.shuffle(s)\n",
    "Cells=Cells[s]\n",
    "labels=labels[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=len(np.unique(labels))\n",
    "len_data=len(Cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encoding\n",
    "Here the problem has two classes so last output layer of neural network will have 2 neurons one for each class, One hot encoding will help us to change labels in binary format. example: 2 can be represented as [1 0] if output layer has 2 neurons and [0 0 1 0] if output has 4 neurons/classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = keras.utils.to_categorical(labels, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data\n",
    "Do Train/Test Split of data and labels that prepared in early section. Classes are defined as the unique labels in the data. Here it will be 2 as Parasitized:0 and Uninfected:1, here 0 and 1 are the mapping in labels for these two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,x_test)=Cells[(int)(0.1*len_data):],Cells[:(int)(0.1*len_data)]\n",
    "x_train = x_train.astype('float32')/255 # As we are working on image data we are normalizing data by divinding 255.\n",
    "x_test = x_test.astype('float32')/255\n",
    "train_len=len(x_train)\n",
    "test_len=len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_train,y_test)=labels[(int)(0.1*len_data):],labels[:(int)(0.1*len_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train=keras.utils.to_categorical(y_train,num_classes)\n",
    "#y_test=keras.utils.to_categorical(y_test,num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train an image classification model\n",
    "\n",
    "### Create Sequential Model:\n",
    "Here I will be using Relu{max(0,z)}, You can try tanh/sigmoid/Leaky Relu for finding performance on various activation functions.Our output layer will be softmax activation rather than sigmoid as we have more than one class to classify. softmax activation calculates e^value/sum(all_values_in_axis[0 or 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model=Sequential()\n",
    "model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500,activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2,activation=\"softmax\"))#2 represent output layer neurons \n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model with loss as binary_crossentropy and using adam optimizer you can test result by trying RMSProp as well as Momentum\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the model with min batch size as 20[can tune batch size to some factor of 2^power ] \n",
    "history = model.fit(x_train,y_train,batch_size=20,epochs=20,verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Experiment info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = experiment.start_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the accuracy on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('\\n', 'Test_Accuracy:-', accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log(\"Accuracy\", accuracy[1], description='Test_Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import redirect_stdout\n",
    "model_summary_fileneme = 'modelsummary.txt'\n",
    "\n",
    "with open(model_summary_fileneme, 'w') as f:\n",
    "    with redirect_stdout(f):\n",
    "        model.summary()\n",
    "\n",
    "run.upload_file(model_summary_fileneme, model_summary_fileneme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze model Accuracy and Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy vs Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Acc/Loss')\n",
    "plt.legend(['Acc Train','Acc Validation', 'Loss Train','Loss Validation'],loc='upper left')\n",
    "plt.plot()\n",
    "run.log_image('Accuracy_Loss', path=None, plot=plt, description='Model Accuracy vs Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model_filename = 'cells.h5'\n",
    "model.save(model_filename)\n",
    "run.upload_file(model_filename, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "sihhu"
   }
  ],
  "categories": [
   "how-to-use-azureml",
   "work-with-data",
   "datasets-tutorial"
  ],
  "category": "tutorial",
  "compute": [
   "Remote"
  ],
  "deployment": [
   "None"
  ],
  "exclude_from_index": false,
  "framework": [
   "Azure ML"
  ],
  "friendly_name": "Introduction to labeled datasets",
  "index_order": 1,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "star_tag": [
   "featured"
  ],
  "tags": [
   "Dataset",
   "label",
   "Estimator"
  ],
  "task": "Train"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
